[toc]

## 数据结构
1. 平衡二叉树、B树、B+树、B*树 
https://zhuanlan.zhihu.com/p/27700617




378. 设计模式



## 深度学习

### 优化方法总结：
1. 梯度下降 Gradient Descent

- 随机梯度下降 Stochastic Gradient Descent
	每读入一个数据，便立刻计算cost fuction的梯度来更新参数
	
- 批量梯度下降 Batch Gradient Descent
	用整个训练集的数据计算梯度，对模型参数进行更新
	cost fuction若为凸函数，能够保证收敛到全局最优值；若为非凸函数，能够收敛到局部最优值
	训练速度慢

- 小批量梯度下降 Mini-batch Gradient Descent
	每次从所有训练数据中取一个子集（mini-batch） 用于计算梯度
	调整学习率
	

2. Momentum
	调整梯度，增加历史梯度的影响
    GD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，十分不稳定。
    更新的时候在一定程度上保留之前更新的方向，同时利用之前的梯度方向和当前batch计算得到的梯度方向加权得到最终的更新方向。

3. Adagrad
自动调整学习率
学习率 / 梯度累加 ，随着训练过程，学习率越来越小
梯度越大，学习率越小，两者相互制衡
在训练中自动的对learning rate进行调整

4. RMSProp
Adagrad的改进
p \* 当前梯度 +（1 - p）历史梯度
（学习率 / 梯度累加） 时，可调整当前梯度和历史梯度的占比

5. Adam
Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法
结合Momentum，Adagrad 调整梯度，增加历史梯度的影响，自动调整学习率

### 梯度消失与梯度爆炸
1. 梯度消失：在深层网络中激活函数sigmoid(x)中的最小值和最大值的梯度都趋近于0。神经网络的反向传播逐层将偏导累乘，当网络层数非常深的时候，最后一层产生的偏导就因为累乘了很多趋近0的偏导而越来越小，最终变为0，从而导致层数比较浅的权重w，b没有得到更新。

由于反向传播，靠近输入层的参数学习的很慢， 靠近输出的参数学习的很好

- 解决方法：
	基本思想是每次训练一层神经元，训练时将上一层的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入。

2. 梯度爆炸：梯度更新，偏导数大于1，累成后梯度更新的值过大

- 解决方法：
	设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内


### 激活函数总结：
1. sigmoid

   <img src="https://www.nowcoder.com/equation?tex=%20%0A%5Csigma(x)%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D%0A%20&amp;preview=true" alt="img"  />

   ![img](https://uploadfiles.nowcoder.com/images/20190506/883102456_1557133568312_95DE468C8B6646B244A0879AA6989FCA)

   缺点：梯度消失，输出值的中心值不是0


2. tanh

![img](https://www.nowcoder.com/equation?tex=%5Ctanh%20(x)%3D2%20%5Coperatorname%7Bsigmoid%7D(2%20x)-1&preview=true)

![img](https://www.nowcoder.com/equation?tex=%20%0A%5Ctanh%20x%3D%5Cfrac%7Be%5E%7Bx%7D-e%5E%7B-x%7D%7D%7Be%5E%7Bx%7D%2Be%5E%7B-x%7D%7D%0A&preview=true)

![图片说明](https://uploadfiles.nowcoder.com/images/20190506/883102456_1557133900686_805B5CBCDD8437C484ECFFC68ED1CC80)

它其实是一个简单放大的sigmoid神经元，解决了zero-centered的输出问题

缺点：梯度消失


3. relu

![img](https://www.nowcoder.com/equation?tex=%20%0AR%20e%20L%20U(x)%3D%5Cmax%20(0%2C%20x)%0A&preview=true)

![图片说明](https://uploadfiles.nowcoder.com/images/20190506/883102456_1557134112136_2D182052FC5B7208614C629DFA3FEBF3)

解决了gradient vanishing问题；导数值大，加快收敛；提供神经网络的稀疏表达能力，负半区神经元不进入训练

缺点：部分神经元不可逆转地死亡


### dropout
隐层神经元以概率 p 被随机丢弃，达到对于每个mini-batch都是在训练不同网络的效果，防止过拟合。测试时，每个神经元乘概率 p。

### attention
- Soft Attention是所有的数据都会注意，都会计算出相应的注意力权值，不会设置筛选条件。
- Hard Attention会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。
https://www.cnblogs.com/guoyaohua/p/9429924.html
## 机器学习

### 基础知识点

##### 数据问题



##### 评判标准



##### 方差与偏差

1. 方差
方差：预测值的离散程度
复杂的模型更有可能拟合func，但是会造成预测值得方差增大（类比线性回归）
- 复杂模型容易造成过拟合（overfitting）
- 简单模型有可能欠拟合（underfitting）

2. 偏差
偏差：预测值与真实值的差距（复杂模型有利于偏差减小）

##### 距离计算方法
1. 欧式距离

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552624962066_FD0F7CEFC32E400647C5CDE1DD75F990)


2. 曼哈顿距离



3. 余弦距离

- 余弦相似度 
    即两个向量夹角的余弦，关注的是**向量之间的角度关系**

  cosθ=向量a.向量b/|向量a|×|向量b|

<img src="data_mining.assets/image-20200708105646605.png" alt="image-20200708105646605" style="zoom: 150%;" />


- 余弦距离

		`余弦距离 = 1 - cos（A，B）` 变现了向量之间的角度关系

![image-20200708111721289](data_mining.assets/image-20200708111721289.png)

![image-20200708111757377](data_mining.assets/image-20200708111757377.png)

![image-20200708111819368](data_mining.assets/image-20200708111819368.png)

4. 切比雪夫距离

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552625124490_6A8E8F8E69DEA490E6B3C8F2E5513460)

5. 向量点积

向量点积：反映着两个向量的“相似度”，两个向量越“相似”，它们的点积越大

### PCA原理

PCA ： principal component analysis ( 主成分分析)

求解过程： 降到N维

1. 对特征减去其均值后，求协方差矩阵 

2. 求特征值，特征向量，并从大到小排序

3. 取前N个特征向量，组成矩阵A (特征向量表示方差最大的方向)

4. X_ = A * X

	PCA是一种线性降维方法，通过线性投影将高维数据映射到低维数据中，所期望的是在**方差最大的方向上投影**，使新特征间的相关性较小。
	
	方差最大；垂直投影
	
	- 方差是用来度量单个随机变量的离散程度
	- 协方差则一般用来刻画两个随机变量的相似程度
	- 协方差矩阵，对角线上的元素为各个随机变量的方差，非对角线上的元素为两两随机变量之间的协方差，根据协方差的定义，我们可以认定：协方差矩阵为对称矩阵
	- 相关系数，X、Y的协方差除以X的标准差和Y的标准差

### 线性回归

最小化平方损失函数：

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552636675906_43A0668882583D6F165A23F4DB0A0EEE)

### k - means



### KNN










### 树模型

##### 决策树

##### ID3

![syzuwd5aya](数据挖掘工程师.assets/syzuwd5aya.jpg)

信息增益 = 原数据集的熵总和 - 按特征A划分数据集后的熵总和

信息增益，针对的是数据集中标签的分布

- 缺点：

1. 无法处理连续特征

2. 信息增益偏向于取值较多的特征

   

##### C4.5

- 信息增益比：

![img](https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317133854323-1046650322.png)

不同于 H( D | A )，数据集划分不是标签值，而是特征A的取值：

![img](https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317135209885-3830592.png)

可以抵消一些由于特征取值过多而造成信息增益过大

- 解决缺失值问题：

第一步，计算所有特征的信息增益或者信息增益率的时候，假设数据集一共10000个样本，特征A中缺失了5000个，则无视缺失值，在剩下的5000个特征中计算信息增益（或者信息增益率），最后乘以0.5，思想就是缺失值多的特征通过这种降低权重的方式来体现信息的缺失；

第二部，如果运气不好，正好这个A特征乘0.5之后得到的信息增益或者增益率还是最大的，那么就像西瓜书中提到的那样，存在缺失值的样板按照比例进入分裂之后的新的分支，假设根据特征A分裂得到两个新的分支，一个分支有2000个样本，一个分支有3000个样本，则按照比例2000个缺失值和3000个缺失值样本分别进入两个分支。

- 缺点：

1. 只能用于分类



##### CART

二叉树，遍历所有特征的特征值

- 基尼指数： 越小纯度越高

![img](https://images2015.cnblogs.com/blog/1094293/201703/1094293-20170317141739932-1276625834.png)

![1114vl45r9](数据挖掘工程师.assets/1114vl45r9.png)

回归：

![m5ty02y7mb](%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%B7%A5%E7%A8%8B%E5%B8%88.assets/m5ty02y7mb.jpg)

c1，c2是所在节点的y均值

二元分类，找到最佳分割点，减低混乱度，使得分裂后的数据集内部的 y 值和均值差的平方的和最小



预剪枝：在训练中进行剪枝，限制分裂后的误差，分裂后的节点样本数

后剪枝：在测试集上，从下往上不断剪枝，计算模型的损失



**模型树:**

对每个分裂点做线性回归，计算平方损失，作为评判分裂点好坏的标准

每个叶子节点包含一个线性方程，将回归分为多个线段





##### GBDT

1. 回归树





2. 分类树

   由多个CART回归树组成，拟合负梯度





### LR

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552624194159_4C51583919397A517272634BF6F96B98)

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552624220293_576FE1589F3DEF2EEACA159D7FA1C5A7)

##### LR实现多分类
方式一:修改逻辑回归的损失函数，softmax分类模型会有相同于类别数的输出，输出的值为对于样本属于各个类别的概率，最后对于样本进行预测的类型为概率值最高的那个类别。

![img](https://pic4.zhimg.com/80/v2-1d4e46217f6df995854b01229434f98c_720w.jpg)

![img](https://pic4.zhimg.com/80/v2-ab690c0473c9056d1024316661bd89da_720w.jpg)

损失函数：最大化正确类别的概率

方式二:根据每个类别都建立一个二分类器

### SVM




### 集成学习

##### bagging和boosting的区别

1. Bagging：对训练集抽样组成每个基模型所需要的子训练集，然后对所有基模型预测的结果进行投票产生最终的预测结果。
   Boosting中基模型按次序进行训练，每个基模型的训练集基于上一个基模型的训练结果，最后将多个基分类器组合。

2. Bagging的各个目标函数可以并行生成，Boosting的各目标函数只能顺序生成。

3. 样例权重：Bagging的各个训练集样例没有权重，而Boosting是有权重的。
   目标函数权重：Bagging：目标函数的权重相等。Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
   
4. Bagging中的基模型为强模型(强模型拥有低偏差高方差)。
   Boosting中的基模型为弱模型。

5. bagging 是减少方差 ，而 boosting 是减少偏差
   bagging 随机采样训练集并对多个基模型取平均可以降低方差；boosting随着迭代不断进行，模型的 bias 会不断降低

##### bagging
- Bootstrap方法
从一个数据集中有放回的抽取，Bagging算法基于bootstrap





##### boosting



1. AdaBoost





2. XGBoost





3. LightGBM








## SQL

```sql
select a.Score as Score,
(select count(distinct b.Score) 
from Scores b    
where b.Score >= a.Score) as `Rank` -- 选择每一行元素作比较

from Scores as a
order by a.Score DESC

---
select c.Name as Department, a.Name as Employee, a.Salary
from Employee a join Department c
on a.DepartmentId = c. Id 
where 
3 > (select count(distinct b.Salary)
from Employee b
where b.Salary > a.Salary and a.DepartmentId = b.DepartmentId
)
```

## 大数据
MapReduce

## 面试

1. 你三年的职业规划是什么？
在这三年中，在努力中提高自己的专业能力，争取成为领域专家。

2. 你和周围的人比你的优势在哪？
更喜欢刨根问底，搞清问题本质，发散思维，喜欢创新

3. 























