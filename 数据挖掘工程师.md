[toc]

## 数据结构
1. 平衡二叉树、B树、B+树、B*树 
https://zhuanlan.zhihu.com/p/27700617

## 牛客刷题
1. SGD,Momentum,Adagard,Adam原理


## 深度学习

##### 优化方法总结：
1. 梯度下降 Gradient Descent

- 随机梯度下降 Stochastic Gradient Descent
	每读入一个数据，便立刻计算cost fuction的梯度来更新参数
	
- 批量梯度下降 Batch Gradient Descent
	用整个训练集的数据计算梯度，对模型参数进行更新
	cost fuction若为凸函数，能够保证收敛到全局最优值；若为非凸函数，能够收敛到局部最优值
	训练速度慢

- 小批量梯度下降 Mini-batch Gradient Descent
	每次从所有训练数据中取一个子集（mini-batch） 用于计算梯度
	调整学习率
	

2. Momentum
	调整梯度，增加历史梯度的影响
    GD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，十分不稳定。
    更新的时候在一定程度上保留之前更新的方向，同时利用之前的梯度方向和当前batch计算得到的梯度方向加权得到最终的更新方向。

3. Adagrad
自动调整学习率
学习率 / 梯度累加 ，随着训练过程，学习率越来越小
梯度越大，学习率越小，两者相互制衡
在训练中自动的对learning rate进行调整

4. RMSProp
Adagrad的改进
p \* 当前梯度 +（1 - p）历史梯度
（学习率 / 梯度累加） 时，可调整当前梯度和历史梯度的占比

5. Adam
Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法
结合Momentum，Adagrad 调整梯度，增加历史梯度的影响，自动调整学习率

##### 激活函数总结：
1. sigmoid
1/(1 + e^-x)



## SQL

```sql
select a.Score as Score,
(select count(distinct b.Score) 
from Scores b    
where b.Score >= a.Score) as `Rank` -- 选择每一行元素作比较

from Scores as a
order by a.Score DESC

---
select c.Name as Department, a.Name as Employee, a.Salary
from Employee a join Department c
on a.DepartmentId = c. Id 
where 
3 > (select count(distinct b.Salary)
from Employee b
where b.Salary > a.Salary and a.DepartmentId = b.DepartmentId
)
```